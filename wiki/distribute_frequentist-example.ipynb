{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python [conda env:py35]",
      "name": "conda-env-py35-py"
    },
    "language_info": {
      "file_extension": ".py",
      "nbconvert_exporter": "python",
      "version": "3.5.2",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "name": "python",
      "mimetype": "text/x-python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab  1 - A frequentist example\n",
        "\n",
        "## Learning Aims\n",
        "\n",
        "- using scipy.stats\n",
        "- MLE\n",
        "- bootstrap\n",
        "- pandas\n",
        "- matplotlib histogram considerations\n",
        "\n",
        "$\\newcommand{\\Ex}{\\mathbb{E}}$\n",
        "$\\newcommand{\\Var}{\\mathrm{Var}}$\n",
        "$\\newcommand{\\Cov}{\\mathrm{Cov}}$\n",
        "$\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}$\n",
        "$\\newcommand{\\indic}{\\mathbb{1}}$\n",
        "$\\newcommand{\\avg}{\\overline}$\n",
        "$\\newcommand{\\est}{\\hat}$\n",
        "$\\newcommand{\\trueval}[1]{#1^{*}}$\n",
        "$\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}$\n",
        "\n",
        "## Installing Python\n",
        "\n",
        "We shall be using Python 3.5 in this course.\n",
        "\n",
        "### Installation\n",
        "\n",
        "Download Anaconda from here:\n",
        "https://www.continuum.io/downloads\n",
        "\n",
        "There are two ways to do this. You could choose to install either the 3.5 version or the 2.7 version.\n",
        "\n",
        "If you chose to install the 3.5 version you are done. \n",
        "\n",
        "### The 2.7 version\n",
        "\n",
        "If you chose to install the 2.7 version you have some work to do.\n",
        "\n",
        "Create a conda environment for python 3.5 with anaconda:\n",
        "\n",
        "`conda create -n py35 python=3.5 anaconda`\n",
        "\n",
        "Update it\n",
        "\n",
        "`conda update -n py35 anaconda`\n",
        "\n",
        "This will pull down all the 3.5 stuff.\n",
        "\n",
        "### Running in a conda environment\n",
        "\n",
        "If you now want to run your shiny new python 3.5 environment, you should \"activate it\"\n",
        "\n",
        "`source activate py35`\n",
        "\n",
        "(or, if you have anaconda on a non-standard place and not in your path like i do\n",
        "\n",
        "`source /anaconda/envs/py35/bin/activate py35`\n",
        "\n",
        ")\n",
        "\n",
        "You ought to read more about the conda command. See http://conda.pydata.org/docs/_downloads/conda-cheatsheet.pdf (critical) and https://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/ (to understand the difference between conda and pip and why python has both (we'll use both in this course).\n",
        "\n",
        "You ought to now `conda install seaborn` or `pip install seaborn`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# The %... is an iPython thing, and is not part of the Python language.\n",
        "# In this case we're just telling the plotting library to draw things on\n",
        "# the notebook, instead of on a separate window.\n",
        "%matplotlib inline\n",
        "# See all the \"as ...\" contructs? They're just aliasing the package names.\n",
        "# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "pd.set_option('display.width', 500)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.notebook_repr_html', True)\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"poster\")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 44
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data on the birth of babies"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Forty-four babies -- a new record -- were born in one 24-hour period at\n",
        "the Mater Mothers' Hospital in Brisbane, Queensland, Australia, on\n",
        "December 18, 1997.  For each of the 44 babies, _The Sunday Mail_\n",
        "recorded the time of birth, the sex of the child, and the birth weight\n",
        "in grams. Also included is the number of minutes since midnight for\n",
        "each birth.\n",
        "\n",
        "REFERENCE:\n",
        "Steele, S. (December 21, 1997), \"Babies by the Dozen for Christmas:\n",
        "24-Hour Baby Boom,\" _The Sunday Mail_ (Brisbane), p. 7.\n",
        "\n",
        "\"Datasets\n",
        "and Stories\" article \"A Simple Dataset for Demonstrating Common\n",
        "Distributions\" in the _Journal of Statistics Education_ (Dunn 1999).\n",
        "\n",
        "Columns\n",
        "\n",
        "       1 -  8  Time of birth recorded on the 24-hour clock\n",
        "       9 - 16  Sex of the child (1 = girl, 2 = boy)\n",
        "      17 - 24  Birth weight in grams\n",
        "      25 - 32  Number of minutes after midnight of each birth"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_table(\"data/babyboom.dat.txt\", header=None, sep='\\s+', \n",
        "                   names=['24hrtime','sex','weight','minutes'])\n",
        "df.head()"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 45
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bin the number of births into hourly bins"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df['minsbin'] = df.minutes // 60\n",
        "df.head()"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 46
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Samples vs population\n",
        "\n",
        "But we have never aked ourselves the philosophical question: what is data? **Frequentist statistics** is one answer to this philosophical question. It treats data as a **sample** from an existing **population**.\n",
        "\n",
        "This notion is probably clearest to you from elections, where some companies like Zogby or CNN take polls. The sample in these polls maybe a 1000 people, but they \"represent\" the electoral population at large. We attempt to draw inferences about how the population will vote based on these samples."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing a model: the exponential distribution\n",
        "\n",
        "Let us characterize our particular sample statistically then, using a *probability distribution*\n",
        "\n",
        "The exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous Poisson process.\n",
        "\n",
        "It takes the form:\n",
        "$$\n",
        "f(x;\\lambda) = \\begin{cases}\n",
        "\\lambda e^{-\\lambda x} & x \\ge 0, \\\\\n",
        "0 & x < 0.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "From Wikipedia: *In probability theory, a Poisson process is a stochastic process which counts the number of events and the time that these events occur in a given time interval. The time between each pair of consecutive events has an exponential distribution with parameter $\\lambda$ and each of these inter-arrival times is assumed to be independent of other inter-arrival times. The process is named after the French mathematician Sim\u00e9on Denis Poisson and is a good model of radioactive decay, telephone calls and requests for a particular document on a web server, among many other phenomena.*\n",
        "\n",
        "In our example above, we have the arrival times of the babies. There is no reason to expect any specific clustering in time, so one could think of modelling the arrival of the babies via a poisson process.\n",
        "\n",
        "Furthermore, the Poisson distribution can be used to model the number of births each hour over the 24-hour period.\n",
        "\n",
        "### What does the exponential distribution look like?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x, l: l*np.exp(-l*x)*(x>0)\n",
        "xpts=np.arange(-2,3,0.05)\n",
        "plt.plot(xpts,f(xpts, 2),'.');\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"exponential pdf\")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 47
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: **some of the code, and ALL of the visual style for the distribution plots below was shamelessly stolen from https://gist.github.com/mattions/6113437/ **."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import expon\n",
        "\n",
        "x = np.linspace(0,4, 100)\n",
        "colors=sns.color_palette()\n",
        "\n",
        "lambda_ = [0.5, 1, 2, 4]\n",
        "plt.figure(figsize=(12,4))\n",
        "for l,c in zip(lambda_,colors):\n",
        "    plt.plot(x, expon.pdf(x, scale=1./l), lw=2, \n",
        "                color=c, label = \"$\\lambda = %.1f$\"%l)\n",
        "    plt.fill_between(x, expon.pdf(x, scale=1./l), color=c, alpha = .33)\n",
        "    \n",
        "plt.legend()\n",
        "plt.ylabel(\"PDF at $x$\")\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.title(\"Probability density function of an Exponential random variable;\\\n",
        " differing $\\lambda$\");"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 48
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How would we draw from this distribution?\n",
        "\n",
        "Lets use the built in machinery in `scipy.stats`:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import expon\n",
        "plt.plot(xpts,expon.pdf(xpts, scale=1./2.),'.')\n",
        "plt.hist(expon.rvs(size=1000, scale=1./2.), normed=True, alpha=0.5, bins=50);\n",
        "#if you remove normed=True you will get the actual number of samples\n",
        "plt.xlabel(\"x\")\n",
        "plt.title(\"exponential pdf and no. of samples(normalized)\");"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 49
    },
    {
      "cell_type": "markdown",
      "source": [
        "In `scipy.stats`, you can alternatively create a frozen object, which holds values of things like the scale"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "rv = expon(scale=0.5)\n",
        "plt.plot(xpts,rv.pdf(xpts),'.')\n",
        "plt.hist(rv.rvs(size=1000), normed=True, alpha=0.5, bins=30);\n",
        "plt.plot(xpts, rv.cdf(xpts));\n",
        "plt.xlabel(\"x\")\n",
        "plt.title(\"exponential pdf, cdf and samples(normalized)\");"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 50
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Poisson distribution\n",
        "\n",
        "The Poisson distribution is another discrete distribution, it expresses the probability of a given number of events occurring in a fixed interval of time (or space, volume, etc.). One assumption made is that these events occur with a known average rate and independently of each other. An example is the number of electrons detected by a sensor in an electron microscope during a time interval, or the number of soldiers in the Prussian army killed accidentally by horse kicks [(see here)](http://en.wikipedia.org/wiki/Poisson_distribution).\n",
        "\n",
        "The Poisson distribution is defined as:\n",
        "\n",
        "\n",
        "$$ f(k; \\mu)= \\frac{\\mu^k e^{-\\mu}}{k!}, $$\n",
        "\n",
        "where $k$ is the number of events, $\\mu$ is a positive real number, and $e$ is Euler's number ($e = 2.71828 \\ldots$)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import poisson\n",
        "# generate samples for different values of mu\n",
        "kpts=np.arange(0,25)\n",
        "for mu, c in zip([1,2, 4, 6], sns.color_palette()[:4]):\n",
        "    randomVariates = poisson.rvs(mu, size=1000)\n",
        "    plt.hist(randomVariates, normed=True, color=c, alpha=0.2, bins=range(0,26), label='$\\mu=' + np.str(mu) + '$')\n",
        "    plt.plot(kpts, poisson.pmf(kpts, mu), '.', color=c)\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Poisson Distribution\")\n",
        "plt.xlabel(\"Number of Events\")\n",
        "plt.ylabel(\"Normed Counts\");"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 42
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding our data using a  histogram-plotted distribution\n",
        "\n",
        "Lets play with our data a bit to understand it:\n",
        "\n",
        "The first birth occurred at 0005, and the last birth in the 24-hour period at 2355. Thus the 43 inter-birth times happened over a 1430-minute period, giving a theoretical mean of 1430/43 = 33.26 minutes between births.\n",
        "\n",
        "Lets plot a histogram of the inter-birth times"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "timediffs = df.minutes.diff()[1:]\n",
        "print(timediffs.mean())\n",
        "timediffs.hist(bins=20, normed=True);"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 93
    },
    {
      "cell_type": "markdown",
      "source": [
        "And do the same for the poisson"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "poiskcount = df.groupby('minsbin')['minutes'].count()\n",
        "poiskcount"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 52
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exponential distribution is a continuous distribution and has a pdf.  The default  `normed=True` option for histograms in matplotlib aims to give us a normalized density by setting the area of the histogram to 1. We can play with the number of bins: this is a bit of an art. Too few and you under-represent variability/ Too many and you overfit to it. Play to see what works for you. I will sometimes use `sns.kdeplot` and  try and adjust bins to match.\n",
        "\n",
        "In this course you will be binning samples very often, so its important to get a feel for this.\n",
        "\n",
        "Since the Poisson is a discrete distribution, it has a probability mass function. Normalizing a pdf is not what we want here, rather, the values of the pmf (which are probabilities) should sum to 1. So we take matters into our own hands.\n",
        "\n",
        "We make sure that every point has equal mass, so that the total mass is one. Then the mass at any point depends on how many samples  (the count) we have for it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.ones_like(poiskcount)/len(poiskcount)\n",
        "poiskcount.hist(weights=weights)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 94
    },
    {
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 75
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maximum Likelihood Estimation\n",
        "\n",
        "\n",
        "One of the techniques used to estimate such parameters in frequentist statistics is **maximum likelihood estimation**. Briefly, the idea behind it is:\n",
        "\n",
        "The product \n",
        "\n",
        "$$\n",
        "L(\\lambda) = \\prod_{i=1}^n P(x_i \\mid \\lambda)\n",
        "$$\n",
        "\n",
        "gives us a measure of how likely it is to observe values $x_1,...,x_n$ given the parameters $\\lambda$. Maximum likelihood fitting consists of choosing the appropriate \"likelihood\" function $L=P(X \\mid \\lambda)$ to maximize for a given set of observations. How likely are the observations if the model is true?\n",
        "\n",
        "Often it is easier and numerically more stable to maximise the log likelyhood:\n",
        "\n",
        "$$\n",
        "\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i \\mid \\lambda))\n",
        "$$\n",
        "\n",
        "In the case of the exponential distribution we have:\n",
        "\n",
        "$$\n",
        "\\ell(lambda) = \\sum_{i=1}^n ln(\\lambda e^{-\\lambda x_i}) = \\sum_{i=1}^n \\left( ln(\\lambda) - \\lambda x_i \\right).\n",
        "$$\n",
        "\n",
        "Maximizing this:\n",
        "\n",
        "$$\n",
        "\\frac{d \\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0\n",
        "$$\n",
        "\n",
        "and thus:\n",
        "\n",
        "$$\n",
        "\\frac{1}{\\est{\\lambda_{MLE}}} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n",
        "$$\n",
        "\n",
        "which is identical to the simple estimator we used above. Usually one is not so lucky and one must use numerical optimization techniques.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This makes intuitive sense: if you get babies at an average rate of 2 per hour, then you can expect to wait half an hour on average for every baby."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_from_mean = 1./timediffs.mean()\n",
        "print(lambda_from_mean, 1./lambda_from_mean)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 54
    },
    {
      "cell_type": "code",
      "source": [
        "minutes=np.arange(0, 160, 5)\n",
        "rv = expon(scale=1./lambda_from_mean)\n",
        "plt.plot(minutes,rv.pdf(minutes),'.')\n",
        "timediffs.hist(normed=True, alpha=0.5, bins=20);\n",
        "sns.kdeplot(timediffs)\n",
        "plt.xlabel(\"minutes\");\n",
        "plt.xlim([0,200])\n",
        "plt.title(\"Normalized data and model for estimated $\\hat{\\lambda}$\");"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 95
    },
    {
      "cell_type": "markdown",
      "source": [
        "What did we just do? We made a 'point estimate' of the scale or rate parameter as a compression of our data. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Poisson"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of the poisson distribution we have:\n",
        "\n",
        "$$\n",
        "\\ell(lambda) = \\sum_{i=1}^n \\left( k_i ln(\\mu) - \\mu - ln(k_i!) \\right).\n",
        "$$\n",
        "\n",
        "You maximize this now.\n",
        "\n",
        "*your answer here* \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mumle = np.mean(poiskcount)\n",
        "mumle"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "np.std(poiskcount)#note mean != stdev, not a great poisson"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 38
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a similar plot to the exponential for the poisson, with a histogram, a kde, and the pmf."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here\n"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 77
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FREQUENTIST STATISTICS \n",
        "\n",
        "In frequentist statistics, the data we have in hand, is viewed as a **sample** from a population. So if we want to estimate some parameter of the population, like say the mean, we estimate it on the sample.\n",
        "\n",
        "This is because we've been given only one sample. Ideally we'd want to see the population, but we have no such luck.\n",
        "\n",
        "The parameter estimate is computed by applying an estimator $F$ to some data $D$, so $\\est{\\lambda} = F(D)$. \n",
        "\n",
        "\n",
        "**The parameter is viewed as fixed and the data as random, which is the exact opposite of the Bayesian approach which you will learn later in this class. **\n",
        "\n",
        "For the babies, lets assume that an exponential distribution is a good description of the baby arrival process. Then we consider some larger population of babies from which this sample is drawn, there is some true $\\trueval{\\lambda}$ which defines it. We dont know this. The best we can do to start with is to estimate  a lambda from the data set we have, which we denote $\\est{\\lambda}$. \n",
        "\n",
        "Now, imagine that I let you peek at the entire population in this way: I gave you some M data sets **drawn** from the population, and you can now find the mean on each such dataset, of which the one we have here is one.\n",
        "So, we'd have M means. You can think of these means as coming from some fixed parameter by some data drawing process\n",
        "\n",
        "Thus if we had many replications of this data set: that is, data from other days, an **ensemble** of data sets, for example, we can compute other $\\est{\\lambda}$, and begin to construct the **sampling distribution** of $\\lambda$.\n",
        "\n",
        "But we dont."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Sampling Distribution of the rate\n",
        "\n",
        "So, in the babies case, the uncertainty in the parameter estimate can be measured by computing the **sampling distribution** of the estimator. \n",
        "What you are doing is sampling many Data Sets $D_i$ from the true population (which we are not given you will argue, and you are right, but just wait a bit), say M of them, each of size N, from some true model $p(\\cdot|\\trueval{\\lambda})$. We will now calculate M $\\est{\\lambda}_i$, one for each dataset. As we let $M \\rightarrow \\infty$, the distribution induced on $\\est{\\lambda}$ is the sampling distribution of the estimator."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the sampling distribution to put confidence intervals on the estimation of the parameters. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bootstrap\n",
        "\n",
        "Bootstrap tries to approximate our sampling distribution. If we knew the true parameters of the population, we could generate M fake datasets. Then we could compute the parameter (or another estimator) on each one of these, to get a empirical sampling distribution of the parameter or estimator, and which will give us an idea of how typical our sample is, and thus, how good our parameter estimations from our sample are.\n",
        "(again from murphy)\n",
        "\n",
        "But we dont have the true parameter. So we generate these samples, using the parameter we calculated. Or, alteratively, we sample with replacement the X from our original sample D, generating many fake datasets, and then compute the distribution on the parameters as before. \n",
        "\n",
        "We do it here for the mean of the time differences. We could also do it for its inverse, $\\lambda$.\n",
        "\n",
        "### Non Parametric\n",
        "\n",
        "Resample the data! We can then plot the distribution of the mean time-difference in each sample."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "M_samples=10000\n",
        "N_points = timediffs.shape[0]\n",
        "bs_np = np.random.choice(timediffs, size=(M_samples, N_points), replace=True)\n",
        "sd_mean=np.mean(bs_np, axis=1)\n",
        "sd_std=np.std(bs_np, axis=1)\n",
        "plt.hist(sd_mean, bins=30, normed=True, alpha=0.5,label=\"samples\");\n",
        "plt.axvline(timediffs.mean(), 0, 1, color='r', label='Our Sample')\n",
        "plt.legend()"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 83
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above procedure os resampling directly into a 2d array might seem a bit strange, so we repeat the same process split up logically into M replications of size N samples below and ,ake the same plot."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "M_samples=10000\n",
        "N_points = timediffs.shape[0]\n",
        "bs_np = np.ones(shape=(M_samples, N_points))\n",
        "bs_np[0,:].shape"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 84
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(M_samples):\n",
        "    bs_np[i,:] = np.random.choice(timediffs, size=N_points, replace=True)\n",
        "sd_mean=np.mean(bs_np, axis=1)\n",
        "sd_std=np.std(bs_np, axis=1)\n",
        "plt.hist(sd_mean, bins=30, normed=True, alpha=0.5,label=\"samples\");\n",
        "plt.axvline(timediffs.mean(), 0, 1, color='r', label='Our Sample')\n",
        "plt.legend()"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 85
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametric \n",
        "\n",
        "And here we do it in a parametric way. We get an \"estimate\" of the parameter from our sample, and them use the exponential distribution to generate many datasets, and then fir the parameter on each one of those datasets. We can then plot the distribution of the mean time-difference."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "rv = expon(scale=1./lambda_from_mean)\n",
        "M_samples=10000\n",
        "N_points = timediffs.shape[0]\n",
        "bs_p = rv.rvs(size=(M_samples, N_points))\n",
        "sd_mean_p=np.mean(bs_p, axis=1)\n",
        "sd_std_p=np.std(bs_p, axis=1)\n",
        "plt.hist(sd_mean_p, bins=30, normed=True, alpha=0.5);\n",
        "plt.axvline(timediffs.mean(), 0, 1, color='r', label='Our Sample')"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your turn to do the same for the poisson distribution, both the non-parametric and the parametric bootstrap."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here\n"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 91
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here\n"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "execution_count": 92
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "metadata": {
        "collapsed": true
      },
      "execution_count": null
    }
  ]
}